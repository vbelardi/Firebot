{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " # Team 1 : Pierre Massimo Adamini, Valentin Belardi, Julian Vladimir José Ruiz Rodriguez, Zoé Frottier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of content\n",
    "\n",
    "- 1. Environment\n",
    "- 2. Implementation\n",
    "    - 2.1 Vision\n",
    "    - 2.2 Path Planning\n",
    "    - 2.3 Kindnapping\n",
    "    - 2.4 Local Obstacle Avoidance \n",
    "    - 2.5 Kalman Filter\n",
    "    - 2.6 Motion Control\n",
    "    - 2.7 Global Implementation\n",
    "- 3. Overall Project\n",
    "- 4. Conclusion \n",
    "- 5. Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Environment\n",
    "\n",
    "### Project Overview\n",
    "\n",
    "**Firebot**, our robot, must navigate through a city to escape from a fire. The map consists of the following elements:\n",
    "\n",
    "- **Buildings**: Represented as black rectangles. These structures act as obstacles.\n",
    "- **Fire spots**: Represented as red rectangles. These are hazardous areas that the robot must avoid.\n",
    "- **Debris**: Randomly appearing on the roads, acting as temporary obstacles.\n",
    "\n",
    "### Objective\n",
    "\n",
    "The robot's goal is to reach the **exit of the city**, indicated by a green square. To succeed, Firebot must:\n",
    "\n",
    "- Maintain a safe distance from **fire spots**.\n",
    "- Navigate around **buildings** and **debris** as needed. Passing close to these obstacles is acceptable, but proximity to fire should be avoided as much as possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Implementation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Vision\n",
    "\n",
    "### Design Choices\n",
    "\n",
    "For vision processing, we represent obstacles and the goal as **rectangles** on the global path, differentiated by their **colors**. This choice is driven by several factors:\n",
    "\n",
    "1. **Rectangles offer reliable contours**:  \n",
    "   - Rectangular shapes are less sensitive to noise compared to other polygonal shapes, ensuring consistent detection.  \n",
    "   - As all obstacles share the same shape, a single set of parameters for the **Gaussian filter** and **Canny edge detection** is sufficient.\n",
    "\n",
    "2. **Handling different shapes**:  \n",
    "   - Our program focuses on rectangles for simplicity, but adding other polygonal shapes can be done without significant complexity.  \n",
    "   - The robot (Thymio) is represented as a triangle\n",
    "   - Local obstacles are white circles, minimizing the risk of confusing different elements.\n",
    "\n",
    "3. **Color-based differentiation**:  \n",
    "   - By leveraging both shape and color in OpenCV, we build a comprehensive vision system capable of distinguishing between map elements.\n",
    "\n",
    "\n",
    "\n",
    "### Vision System Overview\n",
    "\n",
    "The vision program consists of three main functions:\n",
    "- **`sheet`**: Detects the map and defines its boundaries.  \n",
    "- **`detect_obstacles`**: Identifies and classifies obstacles on the map.  \n",
    "- **`detect_thymio`**: Detects the robot's position and orientation.  \n",
    "\n",
    "Each function processes frames captured by the camera.\n",
    "\n",
    "\n",
    "\n",
    "#### 1. Map Detection: `sheet`\n",
    "\n",
    "The `sheet` function identifies the map, which is represented as a white rectangle on a dark background. The process involves:\n",
    "\n",
    "1. **Preprocessing**:  \n",
    "   - Convert the frame to grayscale.  \n",
    "   - Apply a **Gaussian filter** to reduce noise.  \n",
    "   - Use a threshold to create a binary image, enhancing contrast between the map and the background.\n",
    "\n",
    "2. **Edge Detection**:  \n",
    "   - Apply **Canny edge detection** to detect the map's boundaries.  \n",
    "   - Since the map consists of straight rectangles, the orientation provided by Sobel or Prewitt filters is unnecessary, we therefore priviledge the accuracy and low sensitivity to noise of Canny Detection.\n",
    "\n",
    "3. **Contour Detection**:  \n",
    "   - Use OpenCV’s `findContours` function to extract contours.  \n",
    "   - Select the largest contour, which corresponds to the map.\n",
    "   - Remove all pixels outside the map's contour.\n",
    "\n",
    "**Output**:  \n",
    "The function returns the map’s top-left corner coordinates, height, and width in pixels.\n",
    "\n",
    "\n",
    "\n",
    "#### 2. Obstacle Detection: `detect_obstacles`\n",
    "\n",
    "The `detect_obstacles` function identifies rectangular obstacles on the map. The process involves:\n",
    "\n",
    "1. **Edge Detection**:  \n",
    "   - Similar to the `sheet` function, apply a **Gaussian filter** and **Canny edge detection**, but without thresholding to ensure lighter shades (e.g., light green) are detected.\n",
    "\n",
    "2. **Contour Analysis**:  \n",
    "   - Compute the **perimeter** of each contour.  \n",
    "   - Approximate contours as polygons using OpenCV’s `approxPolyDP` with a tolerance of `0.04 * perimeter`, a value determined experimentally.\n",
    "\n",
    "3. **Color Identification**:  \n",
    "   - Create a mask where pixels inside the polygon are white.  \n",
    "   - Use OpenCV’s `bitwise_and` function to isolate pixels within the polygon.  \n",
    "   - Compute the **mean color** of these pixels.\n",
    "\n",
    "4. **Filter Results**:  \n",
    "   - Retain only black, green, and red rectangles.  \n",
    "   - Exclude small red rectangles to avoid false positives from the robot's LEDs.\n",
    "\n",
    "**Output**:  \n",
    "The function returns a list of rectangles, each represented as a tuple containing the top-left and bottom-right corner coordinates (in pixels).\n",
    "\n",
    "\n",
    "\n",
    "#### 3. Robot Detection: `detect_thymio`\n",
    "\n",
    "The `detect_thymio` function uses the same process as `detect_obstacles`, focusing on black triangles. Once a triangle representing the robot is identified:\n",
    "\n",
    "1. **Calculate Position**:  \n",
    "   - Use trigonometric calculations to find the triangle's center.  \n",
    "\n",
    "2. **Determine Orientation**:  \n",
    "   - Compute the robot's orientation in radians based on its shape.\n",
    "\n",
    "**Output**:  \n",
    "The function returns the robot’s center coordinates and orientation.\n",
    "To see the details of the calculations to obtain the center and the orientation, please refer to the comments on the vision code.\n",
    "\n",
    "\n",
    "### Color Calibration\n",
    "\n",
    "To classify colors (black, green, red), thresholds were determined empirically by testing various shades. This ensures compatibility with the camera under different lighting conditions.\n",
    "\n",
    "\n",
    "\n",
    "### Summary\n",
    "You can see below an example of the obsctacle and thymio detection together, with the margin of the thymio for the red and black obstacles. \n",
    "\n",
    "<img src=\"images/image_cam.jpeg\" alt=\"camera\" width=\"500\">\n",
    "\n",
    "The vision program effectively combines **shape detection** (rectangles, triangles, circles) and **color classification** to create a robust and adaptable system. By leveraging OpenCV functions like `Canny`, `findContours`, and `approxPolyDP`, we ensure accurate and efficient detection of all elements in the environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Path Planning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm Implementation with Dijkstra's Algorithm\n",
    "\n",
    "Once the vision system identifies the map's outline, obstacles, start point, and goal, the data is scaled to the size of the map and registered into tables. For this application, we decided to use **Dijkstra's algorithm**. To implement it, we divided the map space into a grid of cells, with each cell representing an area of $1 \\text{cm}^2$. The map data is registered into this grid, where we compute two representations:  \n",
    "\n",
    "1. **Character Grid**:  \n",
    "   - `w` for walls (black obstacles).  \n",
    "   - `f` for fire (red obstacles).  \n",
    "   - `s` for the start point.  \n",
    "   - `g` for the goal.  \n",
    "\n",
    "2. **Numerical Grid**:  \n",
    "   - Used for Dijkstra's algorithm computations.  \n",
    "\n",
    "#### Map Design and Obstacles\n",
    "\n",
    "The map includes two types of obstacles:  \n",
    "- **Black walls**: Representing regular barriers.  \n",
    "- **Red walls**: Representing fire zones, which require the robot (e.g., Thymio) to navigate with a greater margin.  \n",
    "\n",
    "#### Algorithm Process\n",
    "\n",
    "The algorithm processes the grid as follows:  \n",
    "- **Grid incrementation**: Loops traverse the grid, assigning values in the numerical grid based on the distance of each cell from the start.  \n",
    "- **Path Backtracking**: Once the goal is reached, the path is backtracked from the goal to the start by following the smallest numerical values to determine the shortest path.  \n",
    "\n",
    "#### Special Handling for Fire Zones\n",
    "\n",
    "To account for the features of fire walls, additional rules are implemented:  \n",
    "- **Hazardous Zones**:  Cells directly adjacent to fire walls are marked as \"hazardous\" (`h`) and have a higher traversal cost.  \n",
    "- **Dangerous Zones**:  Cells within a radius of 3 cm from fire walls are marked as \"dangerous\" (`d`).  \n",
    "- **Traversal Cost**:  \n",
    "  - `h` cells increment the traversal cost by 4 (compared to 1 for regular cells).\n",
    "  - `d` cells increment the traversal cost by 2 (compared to 1 for regular cells).  \n",
    "\n",
    "![grid_with_num.png](images/grid_with_num.png)\n",
    "\n",
    "\n",
    "### Optimizing Path Points for Thymio Navigation\n",
    "\n",
    "Initially, we decided to return all the points found in the path to the Thymio robot to compute its motion. However, we discovered that using such a large number of points was inefficient and caused several issues:  \n",
    "\n",
    "- **Smoothness**: The motion was not smooth due to the excessive number of \"reach points\" along the trajectory.  \n",
    "- **Error Recovery**: If the Thymio missed a checkpoint or diverged from the path, for local avoidance for example, it was difficult to determine at which point it should resume following the path.  \n",
    "\n",
    "#### Solution\n",
    "\n",
    "To address these issues, we decided to return only the points where the path changes direction. These points provide the most critical information while minimizing the number of required points, making it easier to reconstruct the complete path and navigate smoothly.\n",
    "\n",
    "In the image below, the global path identified by the algorithm is shown in **light blue**, and the points used for navigation are highlighted in **blue cells**.\n",
    "\n",
    "\n",
    "<img src=\"images/final_fig_glob_nav.png\" alt=\"global_nav\" width=\"1000\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Kidnapping\n",
    "\n",
    "\n",
    "When the robot is **lifted off the ground**, it transitions into a **waiting state**, pausing its actions until it is placed back down. The robot uses its **ground sensor** to detect whether it is lifted or resting on the ground.\n",
    "\n",
    "Once the robot is placed back in a **new random location**, it will:  \n",
    "1. Wait to **recalculate its position**.  \n",
    "2. **Recompute the global path** to the objective.  \n",
    "3. Resume its journey toward the destination.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Local Obstacle Avoidance\n",
    "\n",
    "The robot enters the **Obstacle Avoidance** state when its front proximity sensors detect an obstacle within a specified distance. This state involves two key phases: **turning to avoid the obstacle** and **moving straight to clear the area**.\n",
    "\n",
    "### 1. Turning to Avoid the Obstacle\n",
    "- The robot uses an **artificial neural network** to process input from its front proximity sensors.  \n",
    "- Each sensor value is assigned a weight, and the weighted sum determines the appropriate speed commands for the motors.  \n",
    "- These weights were determined experimentally.  \n",
    "- Due to latency between the USB dongle and the Thymio, a **delay (sleep)** is introduced, followed by motor deactivation within the same loop cycle.  \n",
    "- This solution improves the precision of the robot’s turns, though it results in a **stepwise rotation** that is less smooth.\n",
    "\n",
    "### 2. Moving Straight to Clear the Area\n",
    "- After turning, the robot moves in a **straight line** to ensure it does not return to the obstacle, avoiding potential looping in front of it.\n",
    "\n",
    "### Recovery and Path Resumption\n",
    "After completing these steps, the robot:\n",
    "1. **Retrieves its position**.  \n",
    "2. Resumes the **global path** to its destination.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Kalman Filter\n",
    "\n",
    "### Choices \n",
    "To reduce uncertainty caused by the linearization of our model, we opted for a **non-linear model** and implemented an **Extended Kalman Filter (EKF)**.\n",
    "\n",
    "### Model Definition \n",
    "\n",
    "The state vector of the system is defined as **[x, y, θ]**, representing the robot's position (x, y) and orientation (θ). The control input is defined as **[ωr, ωl]**, the angular velocities of the right and left wheels (in PWM), respectively. The model is then described by the following equations:\n",
    "\n",
    "#### Prediction Equation\n",
    "$$\n",
    "x^+ = f(x,u)\n",
    "$$\n",
    "where:\n",
    "$$\n",
    "u = (\\omega_r, \\omega_l)\n",
    "$$\n",
    "\n",
    "The state transition is given by:\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "    x^+ \\\\\n",
    "    y^+ \\\\\n",
    "    \\theta^+\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "    x + T_s\\cdot \\cos(\\theta)\\frac{\\omega_r + \\omega_l}{2} \\\\\n",
    "    y + T_s\\cdot \\sin(\\theta)\\frac{\\omega_r + \\omega_l}{2} \\\\\n",
    "    \\theta + T_s\\frac{\\omega_r - \\omega_l}{L}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "where **T_s** is the sampling time and **L** is the distance between the wheels.\n",
    "\n",
    "### Extended Kalman Filter\n",
    "\n",
    "Since our model is non-linear, we use the **Extended Kalman Filter**. This requires the computation of the **Jacobian matrix** of the state function, which is given by:\n",
    "\n",
    "$$\n",
    "F(x) = \\begin{pmatrix}\n",
    "    1 & 0 & - T_s\\cdot \\sin(\\theta)\\frac{\\omega_r + \\omega_l}{2} \\\\\n",
    "    0 & 1 &  T_s\\cdot \\cos(\\theta)\\frac{\\omega_r + \\omega_l}{2} \\\\\n",
    "    0 & 0 & 1\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "### State Estimation\n",
    "\n",
    "With the Jacobian matrix, we can estimate the state of the robot using the Kalman filter. If camera measurements are available, we can update our state estimation based on them. Otherwise, we rely on the prediction made by the model.\n",
    "\n",
    "### Noise and Tuning\n",
    "\n",
    "To define the noise matrices **Q** (process noise) and **R** (measurement noise), we performed measurements of the Thymio's error and fine-tuned the values to best match the real noise in our environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Motion Control\n",
    "\n",
    "### Controller Choice\n",
    "We selected a **proportional controller** for motion control, as it sufficiently meets the needs of our application. Since the robot operates at moderate speeds, the complexity of incorporating differential or integral terms was unnecessary.\n",
    "\n",
    "### Angle Calculation\n",
    "<img src=\"images/angle.jpg\" alt=\"angle\" width=\"800\">\n",
    "\n",
    "This diagram illustrates the calculation of the robot's angles:\n",
    "\n",
    "- The **position** and **angle** of the robot are provided by the Kalman filter.\n",
    "- The **target position** is determined by the path planning algorithm.\n",
    "\n",
    "The control is based on the difference between the robot's current angle ($\\alpha$) and the target's angle ($\\beta$), denoted as the angle $\\gamma$.\n",
    "\n",
    "Both $\\alpha$ and $\\beta$ are signed angles, measured in the trigonometric sense. The sign of the angle $\\gamma$ determines the rotation direction:\n",
    "- If $\\gamma > 0$, the robot turns **left**.\n",
    "- If $\\gamma < 0$, the robot turns **right**.\n",
    "\n",
    "Additionally, since the angle ranges from $+\\pi$ to $-\\pi$, we handle the transition by applying a modulo operation with $2\\pi$ when the difference angle ($\\gamma$) crosses this threshold.\n",
    "\n",
    "### Controller Implementation\n",
    "The **motion** function implements the controller. The core of the control logic is based on the difference between the robot's current angle and the target angle.\n",
    "\n",
    "The control law is defined as:\n",
    "\n",
    "$$\n",
    "w_{control} = (angle_{target} - angle_{robot}) \\cdot k_p\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $w_{control}$ is the corrective control speed applied to the motors.\n",
    "- $k_p$ is the proportional gain, determined experimentally.\n",
    "\n",
    "We separate the control into two cases based on the angle difference:\n",
    "\n",
    "1. **When the angle difference is below 35°**:  \n",
    "   The robot moves forward at a constant speed, making small corrections to its trajectory.  \n",
    "   $$ w_{right} = w_{const} + w_{control} $$  \n",
    "   $$ w_{left} = w_{const} - w_{control} $$\n",
    "\n",
    "2. **When the angle difference exceeds 35°**:  \n",
    "   The robot turns solely to adjust its orientation.  \n",
    "   $$ w_{right} = w_{control} $$  \n",
    "   $$ w_{left} = -w_{control} $$\n",
    "\n",
    "Where:\n",
    "- $w_{right}$ is the PWM speed for the right motor.\n",
    "- $w_{left}$ is the PWM speed for the left motor.\n",
    "\n",
    "The value of **$k_p$** was determined through a combination of calculations and experimental tuning. The **35° angle threshold** was chosen as a reasonable value.\n",
    "\n",
    "The **motion** function return the values of $w_{right}$ and $w_{left}$ for input of the kalman filter.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Global Implementation\n",
    "\n",
    "The main code for the project is implemented in this Jupyter Notebook and is structured into several key parts. The **try, except, finally** structure ensures that the camera is properly turned off and the robot is stopped at the end of the program, whether it finishes normally or is interrupted. For performance reasons, we chose not to display the camera feed, as rendering the frame would increase execution time and negatively impact the program's performance.\n",
    "\n",
    "### Code Structure\n",
    "- **Vision, Path Planning, and Kalman Filtering**: These components are defined in separate Python files and imported into the Jupyter Notebook.\n",
    "- **Obstacle Avoidance, Kidnapping,Motion Control and Main**: These parts are implemented directly within Jupyter notebook cells.\n",
    "  \n",
    "To enable communication with the Thymio robot, we use asynchronous communication. The command `await tdmclient.notebook.start()` starts the communication within the Jupyter notebook. Additionally, the `@tdmclient.notebook.sync_var` decorator is used for functions that interact with Thymio's variables, ensuring proper synchronization.\n",
    "\n",
    "The functions that interact with Thymio’s variables—**Motion Control**, **Local Avoidance**, and **Kidnapping**—are defined directly within the notebook to be able to use the decorator.\n",
    "\n",
    "\n",
    "\n",
    "### Initialization\n",
    "- **Variable Initialization**: All necessary variables are initialized.\n",
    "- **Initial Loop**: The first loop waits for the detection of the Thymio and the goal, which are the minimal conditions to start the project.\n",
    "- **Path Calculation**: The path to be followed by the robot is calculated.\n",
    "\n",
    "\n",
    "\n",
    "### Main Loop\n",
    "In the main loop, the robot can be in one of three states:\n",
    "1. **Kidnapped** (refer to the Kidnapping section).\n",
    "2. **Avoid State** (refer to the Obstacle Avoidance section).\n",
    "3. **Global Navigation State** (the default state).\n",
    "\n",
    "The main loop continues executing until the robot reaches the goal, indicated by the boolean variable `arrive`.\n",
    "\n",
    "\n",
    "\n",
    "### Global Navigation\n",
    "- **Thymio Detection**: The system attempts to detect the Thymio. If detected, the position provided by the camera (in pixels) is converted to centimeters. If the Thymio is not detected, the variable `camera` is set to `None`, which is passed as input to the Kalman filter.\n",
    "  \n",
    "- **Execution Time**: The variables `start` and `end` are used to calculate the loop’s execution time. This time varies depending on whether the camera detects the Thymio or not. The execution time is used to update the `Ts` value in the Kalman filter.\n",
    "\n",
    "- **Movement**: The robot moves towards its current target position, given by `path[path_step_index]`, using the `motion` function (see the Motion Control section).\n",
    "\n",
    "- **Next Step Condition**: The robot progresses to the next target position when the difference between the current position and the next target is below another threshold. This threshold was also empirically determined.\n",
    "\n",
    "- **Arrival Condition**: The robot determines that it has arrived at the goal when the difference between its current position and the goal is below a predefined threshold. This threshold was determined experimentally for optimal performance.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Real-Time and Additional Plots\n",
    "For visualizing the robot's current position along the path, we use **matplotlib** and **IPython.display**. These libraries allow the plot to be updated continuously at each loop iteration.\n",
    "\n",
    "The plot includes:\n",
    "- The **path**, represented by black dashed lines.\n",
    "- The **current position of the robot**, indicated by a red cross and lines (from the Kalman filter).\n",
    "- The **position from the camera**, represented by a blue square.\n",
    "- The **variance of the robot's position**, shown as a red ellipse around the red cross (from the Kalman filter).\n",
    "- The **goal position**, shown as a green square.\n",
    "\n",
    "If the robot is kidnapped, the path is recalculated. The plot is cleared, and the new path is displayed.\n",
    "\n",
    "Here is an example of the plot during execution. The horizontal axis represent the x position in cm and the vertical axis the y position in cm. The origin is in the top left corner.\n",
    "\n",
    "<img src=\"images/plot_real_time.jpg\" alt=\"angle\" width=\"800\">\n",
    "\n",
    "\n",
    "The plots do not include obstacles, as implementing this feature was resource-intensive and would increase execution time, negatively impacting performance.\n",
    "\n",
    "\n",
    "### Debugging and Data Logging\n",
    "To assist with debugging, we include a variable called `DEBUG`. When `DEBUG` is set to `True`, various data points are saved, including:\n",
    "- Kalman filter position.\n",
    "- Camera position.\n",
    "- Kalman variance.\n",
    "\n",
    "This data are stored in tables, and additional cells can be used to generate further plots to provide more insight into the variables, which helps in debugging and performance analysis.\n",
    "\n",
    "The additional plots available are : \n",
    "\n",
    "This figure  plots the values of position given by the kalman. It shows the position of the robot given by the kalman. The horizontal axis is the x positions and the vertical axis is the y positions. The axis are in cm and the origin is in the top left corner.\n",
    "\n",
    "<img src=\"images/kalman_pos.jpg\" alt=\"angle\" width=\"500\">\n",
    "\n",
    "\n",
    "This figure plots the values of the position of the robot given by the kalman. The horizontal axis represent the timestep and the vertical axis the position. The blue curve is the x position and the orange curve is the y position. \n",
    "\n",
    "\n",
    "<img src=\"images/plot_pos.png\" alt=\"angle\" width=\"500\">\n",
    "\n",
    "\n",
    "This figure shows the variances of the x_position (in blue), y position (in red) and the robot angle (in green). The horizontal axis represent the timestep and the vertical axis the value of the variances given by the kalman filter. We can see that the variance (elipses) increases slowly when we remove the camera.\n",
    "\n",
    "\n",
    "<img src=\"images/variance2.jpeg\" alt=\"angle\" width=\"500\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Overall Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tdmclient Notebook environment\n",
    "import tdmclient.notebook\n",
    "await tdmclient.notebook.start()\n",
    "#Librairy import\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "\n",
    "#import of global_nav, vision and filter functions\n",
    "from global_nav_V2 import global_navigation_algorithm\n",
    "from clean_vision import detect_thymio, detect_obstacles, sheet\n",
    "from new_kalman_filter import kalman_filter\n",
    "from IPython.display import clear_output\n",
    "from matplotlib.patches import Ellipse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MOTION CONTROL CODE\n",
    "\n",
    "\n",
    "ANGLE_DIFFERENCE=0.6\n",
    "\n",
    "\n",
    "@tdmclient.notebook.sync_var\n",
    "def avance(PWM_R,PWM_L):\n",
    "    '''\n",
    "    Set the target speed to the motor\n",
    "\n",
    "    parameters : \n",
    "    - PWM_R: speed in PWM of the right motor\n",
    "    - PWM_L: speed in PWM of the left motor\n",
    "    \n",
    "    '''\n",
    "    global motor_left_target, motor_right_target\n",
    "    motor_left_target=PWM_L\n",
    "    motor_right_target=PWM_R\n",
    "    return\n",
    "@tdmclient.notebook.sync_var\n",
    "def get_speed():\n",
    "    '''\n",
    "    return the actual speed of the motor \n",
    "    '''\n",
    "\n",
    "    global motor_left_speed,motor_right_speed\n",
    "    return motor_left_speed,motor_right_speed\n",
    "\n",
    "def motion(pos_cible,pos_initial):\n",
    "    '''\n",
    "    P controller, calculate the speed of the motor to go to a position \n",
    "    parameters: \n",
    "    pos_cible: target position [x,y] [cm]\n",
    "    pos_initial: actual_position of the robot and angle: [[x],[y],[alpha]] [cm, rad]\n",
    "    '''\n",
    "    kp_theta=1.4\n",
    "    W_const=100\n",
    "\n",
    "    angle_dir=-np.atan2((pos_cible[1]-round(pos_initial[1][0])),(pos_cible[0]-round(pos_initial[0][0])))\n",
    "\n",
    "    #control wich depedend on the difference between the target angle et the robot angle\n",
    "    angle_diff = angle_dir-pos_initial[2][0]\n",
    "\n",
    "    # if the difference is too high, we need to modulate the angle_diff\n",
    "    if angle_diff>np.pi:\n",
    "        angle_diff=angle_diff-2*np.pi\n",
    "    if angle_diff<-np.pi:\n",
    "        angle_diff=angle_diff+2*np.pi\n",
    "\n",
    "    w_control= kp_theta*(angle_diff)*180/np.pi\n",
    "\n",
    "    #if the angle difference is too high : turn only\n",
    "    if abs(angle_diff)>ANGLE_DIFFERENCE:\n",
    "        w_r = int(w_control)\n",
    "        w_l = int(-w_control)\n",
    "    #if the angle difference is small : go forward and turn \n",
    "    else:\n",
    "        w_r = int(W_const + w_control )\n",
    "        w_l = int(W_const - w_control )\n",
    "    \n",
    "    avance(w_r,w_l)\n",
    "   \n",
    "    return w_r, w_l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOCAL AVOIDANCE  CODE\n",
    "\n",
    "\n",
    "@tdmclient.notebook.sync_var\n",
    "def prox_detection():\n",
    "    \"\"\"\n",
    "    Function to detect proximity obstacles.\n",
    "    Returns True if an obstacle is detected within a specified range.\n",
    "    \"\"\"\n",
    "    trsld_min = 2500 # rang of detection\n",
    "    global prox_horizontal\n",
    "    x = [0] * (len(prox_horizontal)-2)\n",
    "    for i in range(len(x)):\n",
    "        if prox_horizontal[i] > trsld_min:\n",
    "            x[i] = prox_horizontal[i]\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "@tdmclient.notebook.sync_var\n",
    "def motor_str(speed):\n",
    "    \"\"\"\n",
    "    Sets both motors to the same speed to move the robot straight when into local avoidance state.\n",
    "    Args:\n",
    "        speed : Speed value which is sent to the motors.\n",
    "    \"\"\"\n",
    "    global motor_left_target, motor_right_target\n",
    "    motor_left_target = speed\n",
    "    motor_right_target = speed\n",
    "    return\n",
    "\n",
    "@tdmclient.notebook.sync_var\n",
    "def motor_turn(speed_L, speed_R):\n",
    "    \"\"\"\n",
    "    Sets the left and right motor speeds to different values for turning the robot when into local avoidance state.\n",
    "    Args:\n",
    "        speed_L : Speed value which is sent to the left motor.\n",
    "        speed_R : Speed value which is sent to the right motor.\n",
    "    \"\"\"\n",
    "    global motor_left_target, motor_right_target\n",
    "    motor_left_target = speed_L\n",
    "    motor_right_target = speed_R\n",
    "    return\n",
    "\n",
    "\n",
    "@tdmclient.notebook.sync_var\n",
    "def obst_avoid():\n",
    "    \"\"\"\n",
    "    Implements the obstacle avoidance state for the thymio using the proximity sensors.\n",
    "        - Uses weighted sensor values to compute motor speeds for obstacle avoidance.\n",
    "        - Continues until obstacles are no longer detected and the robot has moved straight\n",
    "          for a specified duration.\n",
    "    \"\"\"\n",
    "    global prox_horizontal, motor_left_target, motor_right_target, state, not_str, time_front\n",
    "    #Local avoidance constants initialisation\n",
    "    # Weight of sensors\n",
    "    w_l = [40,  20, -20, -20, -40]\n",
    "    w_r = [-40, -20, -20,  20,  40]\n",
    "    sensor_scale = 2500 #scale the wait for the command of the motors\n",
    "\n",
    "    # Stop the motors\n",
    "    motor_str(0)\n",
    "    # Variables pour les calculs\n",
    "    y = [0, 0]\n",
    "    x = [0] * (len(prox_horizontal)-2)\n",
    "\n",
    "    exit_avoid = False #Bool to exit avoid state\n",
    "    str_counter = 0 #Counter for the straight state\n",
    "    while not exit_avoid:\n",
    "        x = prox_horizontal\n",
    "        prox_pres = prox_detection()\n",
    "        if prox_pres:\n",
    "            str_counter = 0\n",
    "            for i in range(len(x)-2):\n",
    "                y[0] += x[i] * w_l[i] //sensor_scale\n",
    "                y[1] += x[i] * w_r[i] //sensor_scale\n",
    "            motor_turn(y[0],y[1]) #Turn the robot until no more obstacle is detected.\n",
    "            time.sleep(0.15)\n",
    "            motor_str(0)\n",
    "        else:\n",
    "            motor_str(200) #Go straight when no obstacle is detected anymore\n",
    "            str_counter += 1\n",
    "            if str_counter > 5:\n",
    "                exit_avoid = True #Bool to exit the local avoidance state\n",
    "        time.sleep(0.15)\n",
    "    motor_str(0)\n",
    "    not_str = 0\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROUND_THRESHOLD=500\n",
    "\n",
    "@tdmclient.notebook.sync_var\n",
    "def ground_detect():\n",
    "    \"\"\"\n",
    "    Detects if the robot is on the ground. \n",
    "    Returns:\n",
    "            - `True` if the ground sensor return a value lower than 500.\n",
    "            - `False` if the ground sensor return a value higher than 500.\n",
    "    \"\"\"\n",
    "    global prox_ground_reflected\n",
    "    if prox_ground_reflected[0] >= GROUND_THRESHOLD:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main Code\n",
    "\n",
    "#COnstants\n",
    "ARRIVAL_DISTANCE=5\n",
    "STEP_DISTANCE=3\n",
    "DEBUG=True\n",
    "#varaible to save data to make plot\n",
    "kalman_pos = []\n",
    "camera_pos = []\n",
    "kalman_P_x = []\n",
    "kalman_P_y = []\n",
    "kalman_P_theta = []\n",
    "\n",
    "def main():\n",
    "    \n",
    "    print(\"Début du programme\")\n",
    "\n",
    "    #constantes \n",
    "    WIDTH_CM=107\n",
    "    HEIGTH_CM=72\n",
    "\n",
    "    #initialisation variable \n",
    "    cap = cv2.VideoCapture(0)\n",
    "    pos_thymio = None\n",
    "    green_rectangles = []\n",
    "    x,y,w,h = 0,0,0,0\n",
    "    #bool for arrival \n",
    "    arrive=False\n",
    "    #step of the path\n",
    "    path_step_index=0\n",
    "    #bool for obstacle detected \n",
    "    avoid_state = False\n",
    "    #save the position of the thymio\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "    #initalization of the plot\n",
    "    fig, ax = plt.subplots(figsize=(15, 10))\n",
    "    line, = ax.plot([], [], '-x', color='red')\n",
    "\n",
    "    #scale axis: x axis is 107 cm, y axis is 72 cm c onvert in m\n",
    "    ax.set_xlim(0,WIDTH_CM)\n",
    "    ax.set_ylim(0,HEIGTH_CM)\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    #display the plot in real time\n",
    "    display(fig)\n",
    "    \n",
    "    \n",
    "    try: \n",
    "        #Initialization, wait for the detection of the thymio and the goal\n",
    "        while pos_thymio == None or len(green_rectangles) == 0:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            x,y,w,h = sheet(frame, cap)\n",
    "            frame = frame[y:y+h, x:x+w]\n",
    "            _, black_rectangles, green_rectangles, red_rectangles = detect_obstacles(frame)\n",
    "            _, pos_thymio, angle_thymio = detect_thymio(frame)\n",
    "\n",
    "            print(\"thymio detected \",pos_thymio)\n",
    "            print(\"goal detected \",green_rectangles)\n",
    "\n",
    "        #calculate the path\n",
    "        path=global_navigation_algorithm(black_rectangles, pos_thymio, green_rectangles, red_rectangles, h, w)\n",
    "\n",
    "        #conversion from pixel to cm\n",
    "        pos_thymio_cm = np.array(pos_thymio)    \n",
    "        pos_thymio_cm[0] = (pos_thymio[0] / w) * WIDTH_CM\n",
    "        pos_thymio_cm[1] = (pos_thymio[1] / h) * HEIGTH_CM\n",
    "\n",
    "        #initialization of the kalman variables\n",
    "        pos_thymio_est=np.array([[pos_thymio_cm[0]],[pos_thymio_cm[1]], [angle_thymio]])\n",
    "        P_est=1000*np.ones(3)\n",
    "        #output of the control \n",
    "        wr = 0\n",
    "        wl = 0\n",
    "\n",
    "        #count the lost of the camera\n",
    "        counter=0\n",
    "        #intialization of the timer for the kalman \n",
    "        start = time.time()\n",
    "\n",
    "        #while the robot is not at the goal\n",
    "        while(not(arrive)):\n",
    "            \n",
    "            #detect if there is an obstacle\n",
    "            avoid_state = prox_detection()\n",
    "            #detect if it is kidnapped\n",
    "            kidnapped = ground_detect()\n",
    "\n",
    "\n",
    "            # ============  KIDNAPPING =================\n",
    "            if kidnapped:\n",
    "                avance(0,0)\n",
    "                print(\"kidnapped\")\n",
    "                time.sleep(3)\n",
    "                pos_thymio = None\n",
    "                #wait to find the thymio\n",
    "                while pos_thymio == None :\n",
    "                    print(\"recherche du thymio\")\n",
    "                    ret, frame = cap.read()\n",
    "                    if not ret:\n",
    "                        break\n",
    "                    frame = frame[y:y+h, x:x+w]\n",
    "                    _,pos_thymio,_=detect_thymio(frame)\n",
    "\n",
    "                #recalculate the path from the new position \n",
    "                path=global_navigation_algorithm(black_rectangles, pos_thymio, green_rectangles, red_rectangles, h, w)\n",
    "                \n",
    "                #reinitialization of the step of the path \n",
    "                path_step_index = 0\n",
    "\n",
    "                # clear the real time plot\n",
    "                ax.cla()\n",
    "                line, = ax.plot([], [], '-x', color='red')\n",
    "                x_data = []\n",
    "                y_data = []\n",
    "                ax.set_xlim(0,WIDTH_CM)\n",
    "                ax.set_ylim(0,HEIGTH_CM)\n",
    "                ax.invert_yaxis()\n",
    "\n",
    "\n",
    "             # ============  LOCAL AVOIDANCE =================\n",
    "            elif avoid_state:\n",
    "                obst_avoid()\n",
    "\n",
    "                #wait to find the thymio\n",
    "                while pos_thymio == None :\n",
    "                    print(\"recherche du thymio\")\n",
    "                    ret, frame = cap.read()\n",
    "                    if not ret:\n",
    "                        break\n",
    "                    frame = frame[y:y+h, x:x+w]\n",
    "                    _,pos_thymio,_=detect_thymio(frame)\n",
    "\n",
    "            # ============ GLOBAL NAVIGATION =================\n",
    "            else:\n",
    "                _,frame=cap.read()\n",
    "                frame = frame[y:y+h, x:x+w]\n",
    "                _,pos_robot_cam,angle_robot_cam=detect_thymio(frame)  \n",
    "\n",
    "                #Update of the position \n",
    "                if pos_robot_cam is None:\n",
    "                    camera = None\n",
    "                    counter += 1\n",
    "                else:\n",
    "                    #conversion pixel to cm\n",
    "                    new_pos_thymio = np.array(pos_robot_cam)\n",
    "                    new_pos_thymio[0] = (float(pos_robot_cam[0]) / w) * WIDTH_CM\n",
    "                    new_pos_thymio[1] = (float(pos_robot_cam[1]) / h) * HEIGTH_CM\n",
    "                    camera=np.array([[new_pos_thymio[0]],[new_pos_thymio[1]], [angle_robot_cam]])\n",
    "                    #add the position saw by the camera on the plot (blue square)\n",
    "                    ax.plot(new_pos_thymio[0], new_pos_thymio[1], 'ks', color='blue')\n",
    "\n",
    "                \n",
    "            \n",
    "                #timer for the execution time between to call of the kalman \n",
    "                end = time.time()\n",
    "                delta = end-start\n",
    "                #call kalman function \n",
    "                pos_thymio_est, P_est  = kalman_filter(wr, wl, camera,pos_thymio_est,P_est, delta) \n",
    "\n",
    "                #start the timer for the execution time between to call of the kalman \n",
    "                start = time.time()\n",
    "            \n",
    "                \n",
    "                #arrival condition \n",
    "                if (abs(pos_thymio_est[0]-path[-1][0]) <= ARRIVAL_DISTANCE and abs(pos_thymio_est[1]-path[-1][1]) <= ARRIVAL_DISTANCE):\n",
    "                    arrive=True\n",
    "                    break\n",
    "                #condition for the next step\n",
    "                if (abs(pos_thymio_est[0]-path[path_step_index][0]) <= STEP_DISTANCE and abs(pos_thymio_est[1]-path[path_step_index][1]) <= STEP_DISTANCE):\n",
    "                    path_step_index=path_step_index+1\n",
    "                # call motion function, take \n",
    "                wr, wl = motion(path[path_step_index],pos_thymio_est)\n",
    "\n",
    "                if DEBUG:\n",
    "                    #save data for the plot\n",
    "                    kalman_pos.append(pos_thymio_est)\n",
    "                    kalman_P_x.append(P_est[0][0])\n",
    "                    kalman_P_y.append(P_est[1][1])\n",
    "                    kalman_P_theta.append(P_est[2][2])\n",
    "\n",
    "                #sleep to let the time for variables updates\n",
    "                # time.sleep(0.1)\n",
    "\n",
    "                # ================== PLOT ==================\n",
    "                \n",
    "                #plot the variance of the position from the kalman in ellipse\n",
    "                ellipse = Ellipse((pos_thymio_est[0], pos_thymio_est[1]), width=2*P_est[0][0], height=2*P_est[1][1], edgecolor='r', facecolor='none')\n",
    "                ax.add_patch(ellipse)\n",
    "                ax.plot([i[0] for i in path], [i[1] for i in path], '--', color='black')\n",
    "                ax.plot(path[-1][0], path[-1][1], 'ks', color='green')\n",
    "                x_data.append(pos_thymio_est[0])\n",
    "                y_data.append(pos_thymio_est[1])\n",
    "\n",
    "                #update plot the position of the thymio in real time(red cross)\n",
    "                line.set_xdata(x_data)\n",
    "                line.set_ydata(y_data)\n",
    "\n",
    "                clear_output(wait=True)\n",
    "                display(fig)\n",
    "\n",
    "        print(\"Arrivé!!!!!\")\n",
    "        avance(0,0)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "            print(\"\\n Keyboard interruption\")\n",
    "\n",
    "    finally:\n",
    "        #close the windows and the camera\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        print(\"Caméra fermée.\")\n",
    "        print(\"nombre de frames sans thymio détecté: \", counter)\n",
    "        avance(0,0)\n",
    "\n",
    "#call function main\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Additional plot for debbuging \n",
    "\n",
    "if(DEBUG):\n",
    "\n",
    "\n",
    "    plt.plot([x[0][0] for x in kalman_pos], [x[1][0] for x in kalman_pos], 'b', label='Path')\n",
    "    plt.legend()\n",
    "\n",
    "    #scale axis: x axis is 107 cm, y axis is 72 cm\n",
    "    plt.xlim(0,107)\n",
    "    plt.ylim(0,72)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(kalman_P_x, 'b', label='P_x')\n",
    "    plt.plot(kalman_P_y, 'r', label='P_y')\n",
    "    plt.plot(kalman_P_theta, 'g', label='P_theta')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot([x[0] for x in kalman_pos], label='Position X')\n",
    "    plt.plot([x[1] for x in kalman_pos], label='Position Y')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Conclusion\n",
    "\n",
    "In this project, we developed an application that enables a robot to autonomously navigate a map with both fixed obstacles of various types and local obstacles that may appear during operation. The robot is designed to handle situations such as being \"kidnapped\" or having a lack of vision.\n",
    "\n",
    "## Challenges\n",
    "One of the main challenges we faced was managing the robot's **real-time performance**. Wireless communication introduces latency, which we had to account for in our system. \n",
    "\n",
    "Another challenge was **camera calibration**, as the camera is highly sensitive to lighting conditions and required careful tuning.\n",
    "\n",
    "The project also involved extensive **parameter tuning**, including:\n",
    "- Tuning the **proportional controller** (the **$k_p$** term),\n",
    "- Adjusting the **Kalman filter noise parameters**,\n",
    "- Setting appropriate thresholds for the **camera detection** and **obstacle avoidance**.\n",
    "\n",
    "## Limitations and Improvements\n",
    "We opted for a simplified approach to **path planning**(as detailed in path plannig section), focusing only on direction changes. As a result, the robot can experience slight deviations between two points. Moreover, we set the **obstacle avoidance threshold** large enough to ensure the robot avoids obstacles without colliding, but this aspect could be further optimized for smoother operation.\n",
    "\n",
    "Initially, we designed the algorithm for a smaller grid (30x30 cells). However, as the project evolved, we ended up increasing the amount of cells in the grid, which reduced the performance of the pathfinding algorithm. Although the performance remained acceptable for our application this experience highlighted the limitations of such algorithm and the importance of choosing the right algorithm based on application-specific parameters.\n",
    "\n",
    "Overall, this project allowed us to gain a deeper understanding of key robotics concepts, including:\n",
    "- **Computer vision**,\n",
    "- **Filtering techniques** (Kalman filter),\n",
    "- **Path planning**,\n",
    "- **Sensor detection**,\n",
    "- **Motor control**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to mention that all the text and information found in the report was wrote by ourselves and we used Chat GPT as a tool to correct the grammar and the spelling as well as to improve readability. No additional content was generated.\n",
    "\n",
    "### Vision : \n",
    "Examples of OpenCV of the Mobile Robotics class\n",
    "https://docs.opencv.org/3.4/d2/d96/tutorial_py_table_of_contents_imgproc.html\n",
    "https://docs.opencv.org/3.4/d7/d16/tutorial_py_table_of_contents_core.html\n",
    "https://docs.opencv.org/3.4/d8/d23/classcv_1_1Moments.html\n",
    "\n",
    "### Kalman : \n",
    "Exercices and theory of Filters of the Mobile Robotics class\n",
    "\n",
    "https://en.wikipedia.org/wiki/Extended_Kalman_filter\n",
    "\n",
    "### Path plannig : \n",
    "Lecture 4/5 on navigation \n",
    "\n",
    "## Main \n",
    "Control Your Thymio\n",
    "\n",
    "## Local Avoidance\n",
    "Obstacle Avoidance exercices and theory of the Mobile Robotics class\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
